* Graphics Programming
** OpenGL
*** What is OpenGL?
OpenGL is considered an API or specification that tells graphic driver developers (NVIDIA, AMD or Intel) what a function
should do and how it should behave.  Because of this, each implementation can be a little bit different, although the
function result has to be the same.

*** OpenGL Websites
API documentation: https://docs.gl/ Learn OpenGL: https://learnopengl.com/Getting-started/OpenGL

*** Core-profile vs. Immediate Mode
Immediate mode is the old way of doing things. Although OpenGL is easier to use with that mode, it's also very
inefficient.  For that reason, we use *Core-profile* mode. The latter offers more flexibility but it's harder.

*** State machine
OpenGL is a big state machine, which means that it always has a context. We can change or use this context by calling
OpenGL functions.

*** 3D space
Everything in OpenGL is in 3D space, but since screens are in 2D there exists a process that transforms 3D coordinates
to 2D. This is done in the [[Graphics Pipeline][Graphics Pipeline]].

*** Graphics Pipeline
It's a series of phases where two main events happen:

1) 3D coordinates are transformed into 2D. This is done at the "first part" of the pipeline.
2) 2D coordinates are transformed into colored pixels. This is done at the "second part" of the pipeline.

The pipeline is composed of different steps or phases, where each one requires as input the output of the previous phase
or step.  The great benefit of this pipeline is that we can parallelize it.  Because graphic cards have a lot of cores,
they can process our data quickly in programs that are called [[Shaders][shaders]].

Initially, we give the pipeline, as input, the [[Vertex Data][vertex data]].

#+CAPTION: Pipeline
#+NAME: Pipeline.png
[[./Pipeline.png]]

The blue background colored phases or steps are the ones where we can inject our own shaders.

Goal of each step:

1. *Vertex Shader*: transform 3D coordinates into "different" 3D coordinates. This shader also allows us to tinker with
   vertex attributes. These coordinates need to be [[Normalized Device Coordinates][normalized]]. Input variables within this shader are usually referred
   to as [[Vertex Attributes][vertex attributes]].
2. *Geometry Shader*: the output of the vertex shader is passed to this one. This shader can generate new shapes using the
   ones we gave it. In the image above, we can see that it creates a second triangle. This shader is _optional_.
3. *Shape Assembly*: the output of the geometry shader or vertex is passed to this one. It takes these vertices or vertex
   (depending on the [[Primitives][primitive]] selected) and assembles all the points.
4. *Rasterization*: takes the output of the shape assembly as input and maps the _resulting primitives_ to actual pixels on
   the screen. The result is called [[Fragment][fragments]], and they're passed to the fragment shader. Before they're passed,
   [[Clipping][clipping]] is performed.
5. *Fragment Shader*: takes fragments as input and its goal is to compute the color for each one of the pixels. In this
   step is where all the magic effects happen.
6. *Tests and Blending*: after the color is determined for each pixel, two more events happen, [[Alpha Test][alpha test]] and
   [[Blending][blending]]. Both can be summarized as a process where opacity and blending of different objects are computed.

In modern OpenGL, we are *required* to define at least one vertex and fragment shader, otherwise the program won't run
correctly.

*** Shaders
They're basically programs that run in the GPU. They need to be created, compiled and when we link them together they
produce a [[Shader Program][shader program]].  Since we need to create shaders, we also need to _delete them_.

A shader typically has the following structure:

#+NAME: Typical shader program
#+BEGIN_SRC glsl
  #version version_number
  in type in_variable_name;
  in type in_variable_name;

  out type out_variable_name;

  uniform type uniform_name;

  void main() {
    // process input and do something...

    // output processed stuff to output variable
    out_variable_name = weird_stuff_processed;
  }
#+END_SRC

We can pass data between shaders (contiguous) if input and output variables have the same name and type. When we link the shaders
together, OpenGL will take care of that.

*** Shader Program
Linked version of multiple shaders combined.  To create a shader program, first, we need to create a program, then
attach the corresponding shaders and finally link them:

#+NAME: Example of creation of shader program.
#+BEGIN_SRC cpp
  unsigned int shaderProgram;
  shaderProgram = glCreateProgram();
  glAttachShader(shaderProgram, vertexShader);
  glAttachShader(shaderProgram, fragmentShader);
  glLinkProgram(shaderProgram);
#+END_SRC

To use it, we call the glUseProgram() function.

*** Vertex Data
It's an array of 3D coordinates (vertices) that should form a triangle.

*** Vertex Format
It refers to all the information (layout of data) associated with each vertex in an 3D object.
It encompasses all the attributes that define a vertex.

*** Vertex Attributes
They are the individual properties of a particular vertex. A vertex can have different properties, for example, its
color and position. We can add more attributes or properties to a vertex.

We have to manually tell OpenGL what's the size in bytes of the information to a particular vertex.

#+CAPTION: Vertex Attributes and Data Example
#+NAME: VertexAttributesAndData.png
[[./VertexAttributesAndData.png]]

In this example:

- Each coordinate is a float (4 bytes, 32 bits).
- Each coordinate has three components: X, Y and Z.
- Values are _tighly packed_.

To inform OpenGL about this, we use the function glVertexAttribPointer.

*** Primitives
Hints we give to OpenGL to tell it how we want to draw our [[Vertex Data][vertex data]].  Some of these hints are:

1. GL_POINTS.
2. GL_TRIANGLES.
3. GL_LINE_STRIP.

*** Clipping
Process that ignores parts or sections of shapes that are not going to be rendered on the screen.  For example, if we're
looking forward, and we have a very extensive image in front of us, clipping will be performed on left and right,
because we can't see it completely.  This process improves performance.

*** Fragment
It's all the data required to render a single pixel on the screen.

*** Alpha Test
Process where object's opacity is determined.

*** Blending
Process where OpenGL determines if one object is in front of another (for example) and blends them accordingly.

*** Normalized Device Coordinates
X, Y and Z coordinates that range from [-1.0, 1.0]. NDC.

*** Vertex Buffer Object (VBO)
It's a chunk of memory that we use store all the vertices of a particular object. It can contain information such as
position, color, normals, etc.

When we use VBOs we also have to tell OpenGL how we want the GPU to interpret the data that's inside, among other things.

Its buffer type is _GL_ARRAY_BUFFER_.

*** GLSL (OpenGL Shading Language)
Programming language used to program shaders in OpenGL.

*** Vectors in GLSL
In GLSL, a vector is composed of 4 components: x, y, z and w. W represents something called _perspective division_.

*** Perspective division
After all vertices coordinates are transformed to [[Clip Space][clip space]], another operation is executed: perspective division. This
operation converts 4D clip space (xw, yw, zw, w) to 3D _NDC_ and it does so by dividing each position vector component
over the W component, which is known as the [[Homogeneous Coordinates][homogeneous coordinate]].

It's about normalizing position vectors.

*** Vertex Array Object (VAO)
It's a container or a manager that stores all the state-related settings needed to render a particular object.

In other words, it stores the current bound [[Vertex Buffer Object][VBO]] for us to use it later.

This is very useful to save us from writing a lot of code, because when we want to draw something to the screen we need
to do a series of steps.

The workflow would look like this:

1) Set up the vertex data, i.e, define vertices for a triangle.
2) Set up the vertex format, i.e, define the layout of the vertex attributes.
3) Create and bind a VAO, that way we save the current context.
4) Link the VAO to the VBO.
5) Render the triangle using the VAO.

We can reuse VAOs (_shared VAOS_) if multiple objects have the same vertex format and rendering settings. However, if
objects need to have, say, different shaders, then we'd need to use more than one VAO.

_Modern OpenGL requires to use at least one VAO._

*** Element Buffer Object (EBO)
They're buffers that store the order (_indices_) in which we want vertices to be drawn.

When we use this approach to draw things, we're doing [[Indexed Drawing][indexed drawing]]. Indices start from 0.

We also need to bind them, just like [[Vertex Buffer Object][VBOs]], however, the main difference is that we'll use GL_ELEMENT_ARRAY_BUFFER when
binding, and then we'll use glDrawElements instead of glDrawArrays.

[[Vertex Array Object (VAO)][VAOs]] also store EBOs.

NOTE:

A VAO stores the glBindBuffer calls when the target is GL_ELEMENT_ARRAY_BUFFER. This also means it stores its unbind
calls so make sure you don't unbind the element array buffer before unbinding your VAO, otherwise it doesn't have an EBO
configured.

*** Indexed Drawing
It's a way of drawing figures on the screen and it works by specifying OpenGL the indices of the vertices that we want
to use to draw our figure. When we do that, OpenGL draws them in the order specified.

For instance, if we have a square, then we have 4 vertices.

To draw two triangles (OpenGL draws triangles), we'd need to specify 6 indices, each one corresponding to a vertex in
the square.

We could specify these indices:

1. top right index
2. bottom right index
3. top left index
--
1. bottom right index
2. bottom left index
3. top left index

The benefits of this approach is that we can be less verbose and save memory.

*** Wireframe Mode
It's a mode that draws only the edges of triangles.

#+NAME: Activating wireframe mode
#+BEGIN_SRC c
  glPolygonMode(GL_FRONT_AND_BACK, GL_LINE);
#+END_SRC

*** Uniforms
Are global variables that live within shaders. They're useful when we need to change something -like a color- every
frame or to pass information between shaders, since they're shared among all shaders that are linked together.

The syntax is:

#+NAME: Example of uniform variable
#+BEGIN_SRC glsl
  uniform vec4 myColor;
#+END_SRC

The way we initialize the uniform from our program/application (and also our CPU) is by invoking the function
glUniform4f (or similar ones):

#+NAME: Example of uniform variable
#+BEGIN_SRC glsl
  int vertexColorLocation = glGetUniformLocation(shaderProgram, "ourColor");
  glUseProgram(shaderProgram);
  glUniform4f(vertexColorLocation, 0.0f, greenValue, 0.0f, 1.0f);
#+END_SRC

_NOTE_:

If you declare a uniform that isn't used anywhere in your GLSL code the compiler will silently remove the variable from
the compiled version which is the cause for several frustrating errors; keep this in mind!

*** Texture Wrapping
When the texture coordinates fall outside the range, there are multiple behaviors available:

- GL_REPEAT: repeats the texture image.
- GL_MIRRORED_REPEAT: same as GL_REPEAT but mirrors the image in each repetition.
- GL_CLAMP_TO_EDGE: clamps between 0 and 1. Stretch effect.
- GL_CLAMP_TO_BORDER: coordinates outside the range are given a user-specified border color.

#+CAPTION: Texture Wrapping Example
#+NAME: TextureWrapping.png
[[./TextureWrapping.png]]

When setting these options, we have the ability to do it per coordinate-axis. Here, x, y and z are called:

- s -> x
- t -> y
- r -> z

#+NAME: Example
#+BEGIN_SRC cpp
  glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_S, GL_MIRRORED_REPEAT);
  glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_T, GL_MIRRORED_REPEAT);
#+END_SRC

*** Texture Filtering
It's how OpenGL determines the color of a pixel on a textured object. When we give a floating point number, for example,
0.5f, then OpenGL looks at nearby [[Texel][texels]] and blends their color together ([[Interpolation][interpolation]]), outputting the final colored
pixel.

There are two types of filtering:

- GL_NEAREST: nearest point or point filtering. It's the default one. OpenGL selects the texel that center is closest to
  the texture coordinate.

#+CAPTION: Nearest Filtering
#+NAME: NearestFiltering.png
[[./NearestFiltering.png]]

- GL_LINEAR: bilinear filtering. OpenGL takes an interpolated value from the neighboring texels, approximating a color.

#+CAPTION: Linear Filtering
#+NAME: LinearFiltering.png
[[./LinearFiltering.png]]

We can also use these two filtering options for [[Magnifying][magnifying]] or [[Minifying][minifying]] operations.

*** Texel
Texture pixel.

*** MipMaps
MipMaps are textures that OpenGL uses when rendering objects that are far away from the viewer.

They're optimized and scaled down (twice per object) and OpenGL uses one or the other depending on the distance from the
viewer to the object.

As OpenGL changes from one texture to another, it may produce visible sharp edges between the two textures.

Example:

#+CAPTION: MipMap
#+NAME: MipMap.png
[[./MipMap.png]]

As with regular textures, we can apply [[Texture Filtering][texture filtering]] too.

Options are:

- GL_NEAREST_MIPMAP_NEAREST: takes nearest mipmap to match the pixel size and uses nearest neighbor interpolation for
  texture sampling.

- GL_LINEAR_MIPMAP_NEAREST: takes nearest mipmap level and samples that level using linear interpolation.

- GL_NEAREST_MIPMAP_LINEAR: linearly interpolates between the two mipmaps that most closely match the size of a pixel
  and samples the interpolated level via nearest neighbor interpolation.

- GL_LINEAR_MIPMAP_LINEAR: linearly interpolates between the two closest mipmaps and samples the interpolated level via
  linear interpolation.

#+NAME: Setting filtering method for mipmaps
#+BEGIN_SRC cpp
  glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_LINEAR_MIPMAP_LINEAR);
  glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_LINEAR);
#+END_SRC

_NOTE_:

A common mistake is to set one of the mipmap filtering options as the magnification filter. This doesn't have any
effect since mipmaps are primarily used for when textures get downscaled: texture magnification doesn't use mipmaps
and giving it a mipmap filtering option will generate an OpenGL GL_INVALID_ENUM error code.

*** Texture Unit
It's like a "shelf" where we can store a different number of textures, usually up to 16 (hardware dependant).

GL_TEXTURE_0 -> GL_TEXTURE_15

This means we can use up to 16 textures in a single shader.

*** Coordinate Systems
There are five different coordinates spaces that we use to transform the object's vertices coordinates to [[Normalized Device Coordinates][NDC]].

This is done like that because some computations are easier to do depending on the coordinate system we're in.

OpenGL generally _expects_ to work with [[Normalized Device Coordinates][NDC]] coordinates after each vertex shader run.

#+CAPTION: CoordinateSystems
#+NAME: CoordinateSystems.png
[[./CoordinateSystems.png]]

**** Local Space
In local space, we deal with local coordinates, which are the coordinates we start from and they're relative to the
object's origin point. When we are creating models in Blender, the coordinates that are shown are local coordinates.

**** World Space
To transform from local space coordinates to world space, we use the _model matrix_. Coordinates here are defined relative
to a larger world, which has a origin point, too.

It's the system that we use to place objects in.

**** View Space
To transform from world space coordinates to view space, we use the _view matrix_.

This is also known as the camera space, or view space. Here coordinates are defined relative to the viewer's PoV.

**** Clip Space
To transform from view space coordinates to clip space, we use the _projection matrix_.

Within this coordinate system, coordinates are normalized to a range that goes from -1.0 to 1.0. All the coordinates
that are not between -1.0 and 1.0 will get clipped, which means that they won't get drawn to the screen.

The projection matrix can be a _persective matrix_ or _orthogonal matrix_. Generally we use the persective matrix.

**** Screen Space
The last step is to transform the clip coordinates to screen coordinates.

This is done in a process that's called _viewport transform_, which converts the coordinates between -1.0 and 1.0 to the
coordinate range defined in glViewPort. This range is typically the width and height of the window.

**** Frustum
It refers to the "viewing box" that the projection matrix creates when transforming view space coordinates to clip
space.

**** Perspective
It's the real life effect of seeing farthest objects as smaller and nearest objects as bigger, in very simple terms.

#+CAPTION: Perspective Matrix
#+NAME: PerspectiveMatrix.png
[[./PerspectiveMatrix.png]]

**** Near Plane
It's a component of the view frustrum that defines the closest distance from the viewer at which objects are rendered.

**** Far Plane
It's a component of the view frustrum that defines the largest distance from the viewer at which objects are rendered.

*** Getting the Colour of an Object
We just need to multiply the vector that represents the light source and the object's colour vector.

The result will be the reflected colour.

** Render Loop
The render loop is the main loop of the program where we _typically_ do 3 things: *process keyboard input*, *render stuff* and
finally *draw to the screen*.  Statements executed within this loop are said to be executed in _one frame_.  Because of
this, it's important for functions within this section to be as fast as possible.
** GLSL
It's the language that we use to write shaders. Similar to C.

It contains useful features to work with vectors and matrices.

Types supported:

- uint
- int
- float
- double
- bool

Containers available:

- vectors (n represents the number of components, [2, 4])
  Components are: x, y, z and w.

  Type of vectors:

  - vecn (floats)
  - bvecn (boolean)
  - ivecn (integers)
  - uvecn (unsigned integers)
  - dvecn (double)

- matrices

*** Swizzling
It's a syntax feature of GLSL that allows us to create vectors in a fancy way.

For example:

#+NAME: Typical shader program
#+BEGIN_SRC glsl
  void main() {
    vec2 someVec;
    vec4 differentVec = someVec.xyxx;
    vec3 anotherVec = differentVec.zyw;
    vec4 otherVec = someVec.xxxx + anotherVec.yxzy;
  }
#+END_SRC

*** Functions
Just like C functions, nothing new.

** Interpolation
Imagine we had a right triangle with three vertices.

Each vertex has a different color, for example, red, green and blue.

Now, within the triangle there are a lot of fragments or pixels that are in-between these vertices, and because a change
of color need to happen as we approach to one of the vertices, OpenGL computes a mix of colors with a smooth transition
for each fragment. That process is called interpolation.

For example, if we look from the red vertex to the green one, we'd see the color yellow exactly in the middle of the
trajectory. As we approach the green vertex, the color will smoothly be converting to green.

** Texture
It's an image that can be 1D, 2D or 3D that we use to add detail to an object.

Coordinates go from (0,0), which is lower left corner, to (1,1) upper right corner.

Example:

#+CAPTION: Texture Coordinates Example
#+NAME: TextureCoordinates.png
[[./TextureCoordinates.png]]

What if we go outside this range? See [[Texture Wrapping][texture wrapping]].

** Sampling
It refers to the process of obtaining the texture color from the texture's coordinates.

** Magnifying
Scaling up a texture.

** Minifying
Scaling down a texture.

** Lighting
*** Colours
They're represented with red, green and blue (RGB) colours.

Values range from [0, 1].

How do colours work? In very simple terms and skipping technicalities, objects in real life don't have a colour
themselves. What we perceive as the object colour is the reflection of a light source to that object.

In other words, when light hits an object, certain wavelengths (or colors) of the light spectrum are absorbed by the
object, while others are reflected. The colors that are reflected are the ones we perceive.

_NOTE_: when doing lighting computations remember to always normalize the relevant vectors since it simplifies a lot of
calculations!

*** Phong Lighting Model
It's a model that helps us represent light effects in Graphics Programming.

This model considers three types of lighting:

- [[Ambient Lighting][Ambient Lighting]]: objects in real life aren't completely dark. There's usually some light somewhere like the moon or a
  distant light that "gives colour to objects". We simulate this effect by provoding an ambient lighting, which is just
  a constant.

- [[Diffuse Lighting][Diffuse Lighting]]: simulates the directional impact of the light source to an object.

- [[Specular Lighting][Specular Lighting]]: simulates the bright part of a shiny object when the light source reaches it. Its colour tends to
  be more like the light source colour rather than the object's.

*** Ambient Lighting
In games and reality, objects don't turn completely black. There's always a source of light somewhere. That could be the
sun or a distant light.

Ambient lighting tries to simulate this effect. We don't care about object's positions at all.

What we need to compute ambient lighting is:

- A constant representing the ambient factor.
- Light's and object's colours.

And that's it.

#+NAME: Typical shader program
#+BEGIN_SRC glsl
  void main() {
    vec3 ambient = (light.colour * light.ambient) * vec3(texture(material.diffuse, TexCoords)).rgb;
  }
#+END_SRC

*** Diffuse Lighting
It's the process of giving more light to objects the more they're _aligned_ with the light source direction vector.

When computing diffuse lighting we need to get the angle between the light source ray and the object's fragment.
To get the _light source ray vector_, we need to _subtract the vector position of the light from the vector position of the
fragment's coordinates_.

The other vector we need is called a [[Normal Vector][normal vector]], and it's just a normalized vector that is perpendicular to the
object's surface. We provide this data in the vertex data.

Regarding the angle they form, the lower the angle, the more impact light will have. See this image:

#+CAPTION: Diffuse Lighting
#+NAME: DiffuseLighting.png
[[./DiffuseLighting.png]]

Once we have both vectors, we compute the _dot product between the normal and the light direction vector_. They both have
to be normalized first! This product will produce the cosine value, which is the value that we use later to get the
diffuse vector.

Something like this:

#+NAME: Diffuse Lighting Computation
#+BEGIN_SRC glsl
  void main() {
    vec3 lightDirection = normalize(light.position - FragmentWorldSpaceCoordinates);
    // Diffuse
    vec3 normal = normalize(Normal);
    float diff = max(dot(normal, lightDirection), 0.0);
    // vec3(1.f, 1.f, 1.f) is light's colour.
    vec3 diffuse = (light.diffuse * vec3(1.f, 1.f, 1.f)) * diff * vec3(texture(material.diffuse, TexCoords)).rgb;
  }
#+END_SRC

*** Specular Lighting
Like [[Diffuse Lighting][diffuse lighting]], this type of lighting depends on the _light direction vector_ and on the normal vector of the
fragment. However, this one also takes into account the _viewer's direction vector_.

Specular lighting tries to simulate the light reflection that some materials produce, for example, steel.

So, we need:

- Light's direction vector: computed subtracting light's position vector from fragment's position vector.
- Fragment's normal vector, which is perpendicular to it. We provide it in vertex data.
- Viewer's direction vector: computed subtracting viewer's direction vector from fragment's position vector.
- Reflection vector: computed by reflecting the light's direction vector over the normal vector.

Visually, we have something like this:

#+CAPTION: Specular Lighting
#+NAME: SpecularLighting.png
[[./SpecularLighting.png]]

As we can see from the image, the closer the angle between light's reflection vector and the viewer's direction vector,
the highest the impact of the reflection will be.

To compute the specular factor, we perform _a dot product between viewer's direction vector and light reflection
vector_. Then we use that scalar or factor to multiply it with light's specular intensity (we give this value) and
light's colour.

#+NAME: Specular Lighting Computation
#+BEGIN_SRC glsl
  void main() {
      vec3 viewerDirection = normalize(viewerPosition - FragmentWorldSpaceCoordinates);
      vec3 reflectionDirection = reflect(-lightDirection, normal);
      float spec = pow(max(dot(viewerDirection, reflectionDirection), 0.0), material.shininess);
      // vec3(1.f, 1.f, 1.f) is light's colour.
      vec3 specular = (light.specular * vec3(1.f, 1.f, 1.f)) * spec * vec3(texture(material.specular, TexCoords)).rgb;
      specular *= attenuation;
      specular *= intensity;
  }
#+END_SRC

*** Blinn-Phong Model
TODO

*** Global Illumination Algorithms
Algorithms that compute light's behavior when light particles bounce and scatter all around, taking into consideration
reflection on objects and much more stuff. They're expensive to compute, though, that's why we use a simple algorithm
for our scenes.

*** Materials
Real world objects have different light reflection properties. For example, a wooden container doesn't reflect light in
the same way a steel container does.

In order to simulate that behavior, we define materials for each surface and lighting type, that is, one material for
ambient lighting, one for diffuse lighting and another one for specular lighting.

*** Lighting Maps
They're [[Materials][material]] textures that we use to give a more realistic effect/look when light hits our objects. There are
different types of lighting maps:

**** Diffuse Maps
They're textures that are used to give [[Diffuse Lighting][diffuse lighting]] properties per [[Texel][texel]]. These maps are applied to the object's
surface during rendering to simulate the interaction with light with the material.

**** Specular Maps
They're textures that are used to give [[Specular Lighting][specular lighting]] properties per [[Texel][texel]]. These maps are applied to the object's
surface during rendering to simulate the interaction with light with the material. Specular maps give that shiny
behavior on objects.

*** Light Casters
They're light sources in our scene.

There are different types of light casters:

**** Directional Light
This light caster tries to simulate very powerful light sources that no matter how far they are from objects, they're
still illuminating them almost completely. For example, the sun or a powerful bulb in a small room.

What we care about when doing directional light is:

- Light colour & position.
- Object position and its normals.

With this information, then we proceed to compute [[Phong Lighting Model][The Phong Lighting Model]] and that's it.

**** Point Light
Tries to simulate light that comes from sources but that isn't too powerful. For example, a desktop lamp or a
streetlight. When we say "not too powerful" we mean that the light attenuates as it travels through space.

So, we care about:

- Light colour & position.
- Object position and its normals.
- Attenuation factor, given by the formula: 1 / Kc * Kl * d * Kq * d^2.

Where:

- Kc: generally it's value is 1 to make sure that the denominator never gets smaller than 1, because that would increase
  the intensity a lot for certain distances. It's called the _constant term_.
- Kl: it's called the _linear term_.
- Kq: it's called the _quadratic term_.

To choose the right values we more or less follow the next table:

#+CAPTION: Point Light Table Values
#+NAME: PointLightTableValues.png
[[./PointLightTableValues.png]]

Using this formula with [[Phong Lighting Model][The Phong Lighting Model]] we get a good representation of these type of light casters.

**** Spotlight
Tries to simulate spotlight lights, for example, a flashlight, but also behaving as a [[Point Light][point light]]. That's why it's a
little bit more complicated than the others.

In this case, we want to simulate the circle of light over objects when the light source is pointing at them.

To do this, we need the following information:

#+CAPTION: Spotlight Light
#+NAME: SpotlightLight.png
[[./SpotlightLight.png]]

- LightDir: the vector pointing from the fragment to the light source.
- SpotDir: the direction the spotlight is pointing at.
- Phi: represents the radius of the spotlight. It's called the _cutoff angle_. Everything outside this angle is not illuminated.
- Theta: the angle between LightDir and SpotDir. The value of theta has to be less than Phi's to be inside the spotlight.

How do we compute theta? First, we normalize LightDir and SpotDir. Then, we apply the dot product, which returns the
cosine of the angle, and there we have it. _Remember that SpotDir needs to be negated because we want vectors to point
towards the light source._

Now, we'd have to compare theta and phi. If Theta is inside Phi, then we're in the spotlight.

***** Smooth/Soft Edges
The previous section allows us to compute the spotlight light but the circumference of the light circle may look
unrealistic. For that reason, we generally add a smooth/soft edge effect, which is given by this formula:

#+CAPTION: Smooth Edges
#+NAME: SmoothEdges.png
[[./SmoothEdges.png]]

Where:

- I: intensity.
- E: epsilon, the cosine difference between the inner (phi) and outer cone (gamma).
- Y: gamma, the outer cutoff value.

#+NAME: Smooth Soft Edges Computation
#+BEGIN_SRC glsl
  void main() {
      float theta     = dot(lightDir, normalize(-light.direction));
      float epsilon   = light.cutOff - light.outerCutOff;
      float intensity = clamp((theta - light.outerCutOff) / epsilon, 0.0, 1.0);
      ...
      // we'll leave ambient unaffected so we always have a little light.
      diffuse  *= intensity;
      specular *= intensity;
      ...
  }
#+END_SRC

** Model Loading
For complicated figures, we don't define every vertex manually. We use tools like blender to import these figures/models
to our application.

Models are usually exported using two formats:

- [[Wavefront Format][Wavefront]] .obj.
- [[Collada][Collada]] file format.

*** Assimp
It's a library that we use to load models from files. This is the internal structure when loading a model:

#+CAPTION: AssimpModelStructure.png
#+NAME: AssimpModelStructure.png
[[./AssimpModelStructure.png]]

*** Mesh
It's a combination of primitives that form a shape. When we create models, we don't create a single shape, what we do
instead is create sub-models or shapes that together end up forming the model. Each of these _single shapes_ is called a
mesh.

For example: a human model. To model this, a general approach is to create the different parts (arms, legs, head, etc)
as sub-models or shapes. Each of these shapes is a mesh.

_NOTE_: the term "mesh" can also refer to the combination of all of these shapes.

*** Model
It's a combination of [[Mesh][meshes]] that end up forming the final object that we want to render. For example, a car, a person,
a house, etc.

** Depth Testing
It's a process that consists in determining whether a fragment should be visible or not by considering its depth value
(z component).

When we tell OpenGL to enable depth testing, what OpenGL does is, for every frame, it stores the current fragment's
depth value in the _depth buffer_ if the test succeeds.

We can control the way OpenGL performs this testing using the function glDepthFunc, and we have these options:

- GL_ALWAYS: The depth test always passes.
- GL_NEVER: The depth test never passes.
- GL_LESS: Passes if the fragment's depth value is less than the stored depth value.
- GL_EQUAL: Passes if the fragment's depth value is equal to the stored depth value.
- GL_LEQUAL: Passes if the fragment's depth value is less than or equal to the stored depth value.
- GL_GREATER: Passes if the fragment's depth value is greater than the stored depth value.
- GL_NOTEQUAL: Passes if the fragment's depth value is not equal to the stored depth value.
- GL_GEQUAL: Passes if the fragment's depth value is greater than or equal to the stored depth value.

_NOTE_: it's important to clear the depth buffer every frame.

_NOTE_: Fragments closer to the viewer will typically have lower depth values, while fragments that are farther away will
have higher depth values. Each fragment generated during rendering is associated with a depth value, typically
_representing the distance from the viewer's eye to the fragment's position in the scene_.

**** Early Depth Testing
It's a process that OpenGL can do to improve performance that consists in doing depth testing before the fragment shader
runs. Because the nature of fragment shader's is expensive to do, this saves computational resources. To achieve this we
shouldn't as a general rule modify the Z component in the fragment shader.

**** Depth Value Precision
The thing to keep in mind is that values in the depth buffer are not linear in clip space, however, they're linear in
view space, before the projection matrix is used.

_NOTE_: For more details, go to LearnOpenGL.

**** Z-Fighting
It's the phenomenom that happens when OpenGL cannot tell which object is in front of the other one, causing visual
glitches.

There are 3 ways of avoiding Z-fighting:

- Don't put objects very close to each other. Instead, have a little offset between them that is barely noticeable, so
  OpenGL can evaluate the difference between depths values without problems.
- Increase the near plane distance from the viewer's position, but not too far away because near objects won't get
  rendered because they'll get clipped.
- Increase the depth buffer precision to 32 bits. This has performance implications. Generally, the default buffer
  precision is 24 bits.

** Stencil Testing
It's another process that has the ability to discard fragments when rendering. The stencil test is executed before the [[Depth
Testing][depth test]].

This operation has a buffer attached to it, called the _stencil buffer_, where we can store values from 0-256 (8 bits).

We can update this buffer at will.

This is an example of a stencil buffer:

#+CAPTION: StencilBuffer.png
#+NAME: StencilBuffer.png
[[./StencilBuffer.png]]

In this example, a stencil buffer is cleared with zeroes and then an open rectangle of ones is stored in the stencil
buffer. The fragments on the scene are only rendered (the rest discarded) wherever the stencil value of these fragments
is one.

In general, we use the stencil buffer as follows:

1. Enable writing to the stencil buffer.
2. Render objects, which updates the content of the stencil buffer.
3. Disable writing to the stencil buffer.
4. Render other objects, this time discarding certain fragments depending on the contents of the stencil buffer.

By using the stencil buffer we can thus discard certain fragments based on the fragments of other drawn objects in the
scene.

To use the stencil buffer, we need to activate it first calling: glEnable(GL_STENCIL_TEST).

We also need to clear it every frame: glClear(GL_STENCIL_BUFFER_BIT).

To enable or disabling writing: glStencilMask(0xFF) and glStencilMask(0x00); We can use more masks if we'd like.

To determine how a stencil test should pass or succeed in order to discard a fragment (or not) we have these functions:

- glStencilFunc(func, ref, mask): _func_ sets the test function that determines whether a fragment passes or is
  discarded. This is applied to the stored stencil test value and the _glStencilFunc_'s ref value. Options:
  - GL_NEVER
  - GL_LESS
  - GL_LEQUAL
  - GL_GREATER
  - GL_GEQUAL
  - GL_EQUAL
  - GL_NOTEQUAL
  - GL_ALWAYS
  _ref_: the stencil buffer's content is compared to this value.
  _mask_: mask that is ANDed with the reference value and the stored stencil value before the test compares
  them. Initially set to all ones.

- glStencilOp(sfail, dpfail, dppass): _sfail_ action to take if the stencil test fails.
  _dpfail_: action to take if the stencil test passes, but the depth test fails.
  _dppass_: action to take if both tests pass.

  For each argument, we got the following options:

  - GL_KEEP: The currently stored stencil value is kept.
  - GL_ZERO: The stencil value is set to 0.
  - GL_REPLACE: The stencil value is replaced with the reference value set with glStencilFunc.
  - GL_INCR: The stencil value is increased by 1 if it is lower than the maximum value.
  - GL_INCR_WRAP: Same as GL_INCR, but wraps it back to 0 as soon as the maximum value is exceeded.
  - GL_DECR: The stencil value is decreased by 1 if it is higher than the minimum value.
  - GL_DECR_WRAP: Same as GL_DECR, but wraps it to the maximum value if it ends up lower than 0.
  - GL_INVERT: Bitwise inverts the current stencil buffer value.

By default, glStencilOp is set to (GL_KEEP, GL_KEEP, GL_KEEP), so whatever the outcome of any of the tests, the stencil
buffer will keep its values.

Usage examples: drawing borders (outlining) around models or objects.

** Blending
It's the process of implementing transparency within objects in the scene.

_This process is computed after the fragment shader and after all tests have passed._

To use blending, we need to tell OpenGL to activate it for us: glEnable(GL_BLEND);

The formula that OpenGL uses to compute the final colour of objects when transparency is involved is this one:

#+CAPTION: BlendingFormula.png
#+NAME: BlendingFormula.png
[[./BlendingFormula.png]]

Where:

- _Csource_: it's the source colour vector. This is the output colour of the fragment shader.
- _Fsource_: the source factor value. Sets the impact of the alpha value on the source colour vector.
- _Cdestination_: it's the destination colour vector. This has the contents of the colour buffer.
- _Fdestination_: it's the destination factor value. Sets the impact of the alpha value on the destination vector.

OpenGL allows us to control several things regarding this equation. First, it allows us to define the factor value for
both the source and destination, using the function glBlendFunc(sfactor, dfactor). Possibles values are:

- GL_ZERO: Factor is equal to 0.
- GL_ONE: Factor is equal to 1.
- GL_SRC_COLOR: Factor is equal to the source color vector Csource.
- GL_ONE_MINUS_SRC_COLOR: Factor is equal to 1 minus the source color vector: 1−Csource.
- GL_DST_COLOR: Factor is equal to the destination color vector Cdestination.
- GL_ONE_MINUS_DST_COLOR: Factor is equal to 1 minus the destination color vector: 1−Cdestination.
- GL_SRC_ALPHA: Factor is equal to the alpha component of the source color vector Csource.
- GL_ONE_MINUS_SRC_ALPHA: Factor is equal to 1−alpha of the source color vector Csource.
- GL_DST_ALPHA: Factor is equal to the alpha component of the destination color vector Cdestination.
- GL_ONE_MINUS_DST_ALPHA: Factor is equal to 1−alpha of the destination color vector Cdestination.
- GL_CONSTANT_COLOR: Factor is equal to the constant color vector Cconstant
- GL_ONE_MINUS_CONSTANT_COLOR: Factor is equal to 1 - the constant color vector Cconstant.
- GL_CONSTANT_ALPHA: Factor is equal to the alpha component of the constant color vector Cconstant.
- GL_ONE_MINUS_CONSTANT_ALPHA: Factor is equal to 1−alpha of the constant color vector Cconstant.

We can even set different options for each RGB component using the function glBlendFuncSeparate.

Second, it allows us to define the operation of the equation, using glBlendEquation(mode):

- GL_FUNC_ADD: the default, adds both colors to each other: Cresult = Src + Dst.
- GL_FUNC_SUBTRACT: subtracts both colors from each other: Cresult = Src − Dst.
- GL_FUNC_REVERSE_SUBTRACT: subtracts both colors, but reverses order: Cresult = Dst − Src.
- GL_MIN: takes the component-wise minimum of both colors: Cresult = min(Dst,Src).
- GL_MAX: takes the component-wise maximum of both colors: Cresult = max(Dst,Src).

Generally, things stay at GL_FUNC_ADD.

_NOTE_: when drawing transparent objects in a scene, if we're not careful we can create visual bugs because blending and
depth testing can create conflicts betweem them. For example, if we were to draw multiple transparent windows in a scene
without a particular order, windows that are closer to the viewer but were drawn first will not allow us to see through
them, because the process of depth doesn't take alpha values in consideration, so the transparent window will end up
looking as a opaque object.

To avoid such problems, a common technique is to _sort windows based on their distance between their position and the
camera's_.

Taking into account other objects, the process should look something like this:

1. Draw all opaque objects first.
2. Sort all transparent objects.
3. Draw all transparent objects in sorted (desc, by their distance, i.e farthest ones first, near ones last) order.

Another approach is using _order independent transparency_.

*** Fully Transparent or Not
For some specific cases, for example, putting grass in a scene, it's convinient to just use a .png file image and load
them in a batch-fashion. Because .png files allow for transparency, what OpenGL does (if we tell it so in the fragment
shader, evaluating the alpha channel and _discard_ ing the right fragments) is to ignore fragments whose alpha value is
insignificant.

_NOTE_: When doing this, we might run into the problem of seeing solid fragments in places they shouldn't be in. For
example, in the case of a grass texture, we could see a tiny white border on top of the texture. That's something that
can get fixed by using the option GL_CLAMP_TO_EDGE for textures with GL_RGBA.

** Face Culling
It's an optimization process that OpenGL can do where it doesn't render faces of objects that are not facing the
viewer. For example, if we were facing a cube, we could see at maximum three faces at once. The other three faces (50%)
don't need to be processed and passed to the fragment shader, because we will not see them anyways.

Likewise, if we were seeing only one face of the cube, we wouldn't need to process the other five faces.

The purpose of face culling is to optimize rendering by skipping the rendering of faces that are not visible to the
viewer, thereby reducing unnecessary fragment shader computations.

_How does OpenGL do this?_

The way it does that is by checking if triangles are _front facing_ the viewer. If they are, they will get passed to the
fragment shader. If they're not, they won't.

This front facing and back facing is determined by analyzing the _winding order_ of the vertex data.

When we define the vertices of a triangle, we're definining them in a certain order. That order is what is known as
_winding order_, and it can be _clockwise_ or _counter-clockwise_:

#+CAPTION: WindingOrder.png
#+NAME: WindingOrder.png
[[./WindingOrder.png]]

OpenGL uses the _winding order_ to determine if a triangle is _front facing_ or _back facing_. By default, _triangles defined
with counter-clockwise vertices_ are processed as front-facing triangles.

We need to tell OpenGL to activate face culling with: glEnable(GL_CULL_FACE);

We can also tell OpenGL if we want to cull front, back or front & back faces with: glCullFace(GL_FRONT || GL_BACK ||
GL_FRONT_AND_BACK);

We can also tell OpenGL the way to decide if a triangle is front facing or back facing with: glFrontFace(GL_CCW ||
GL_CW); CCW stands for Counter-Clockwise and CW for Clockwise.

This is the visual effect:

#+CAPTION: WindingOrderFrontAndBack.png
#+NAME: WindingOrderFrontAndBack.png
[[./WindingOrderFrontAndBack.png]]

** Framebuffers
It's a buffer that is stored in the GPU and contains a collection of other buffers, like the _colour buffer_, _stencil
buffer_, _depth buffer_, etc.

OpenGL has two types of framebuffers:

- The default one, provided by OpenGL's context.
- User-created framebuffers, which are called framebuffer objects (FBOs).

User-created framebuffers (FBOs) are used to create off-screen buffers for rendering later. For example, it's a common
approach to use FBOs to render a mirrored world.

_FBOs are not directly visible_, namely, we cannot see them in the window. _Only the default framebuffer_ has a visual
output in our window.

#+NAME: Create a framebuffer example
#+BEGIN_SRC c
  unsigned int fbo;
  glGenFramebuffers(1, &fbo);
  glBindFramebuffer(GL_FRAMEBUFFER, fbo);
#+END_SRC

Generally, we always bind to GL_FRAMEBUFFER, but we have two more options:

- GL_READ_FRAMEBUFFER: we bind to the framebuffer in read-only mode.
- GL_DRAW_FRAMEBUFFER: we bind to the framebuffer in write mode.

It's a good idea to always check if the framebuffer is _completed_ before using it. For it to be complete, we need to:

- Attach at least one buffer (colour, stencil and/or depth).
- There should be at least one colour attachment.
- All [[Attachment][attachments]] should be complete as well (reserved memory), i.e correctly initialized and sized.
- Each buffer should have the same number of [[Samples][samples]] when using [[Multisampling][multisampling]].

#+NAME: Checking if the framebuffer is complete
#+BEGIN_SRC c
  if(glCheckFramebufferStatus(GL_FRAMEBUFFER) == GL_FRAMEBUFFER_COMPLETE)
#+END_SRC

**** Attachment
It's a buffer (memory location) that's used in a framebuffer. We can think of them as _images_. We have two types of
attachments:

- *Textures attachments*: all rendering commands will write to the texture, as if it was a regular colour/depth
  buffer. _A great point_ of using using textures as attachments is that _the render output_ is then stored in a texture
  that we _can later query in our shaders_, like any other texture.

  When creating a texture for a framebuffer attachment, we have to keep in mind some minor details:

  - In the function glTexImage2D, for width and height, we generally put the window's width and height, although this is
    not necessary.
  - _We pass nullptr as the data parameter_.

  _If we want to render the whole screen_ to a texture of a smaller or larger size, we'd need to use glViewport before
  rendering to our framebuffer passing the texture's dimensions. Otherwise, our render commands will only fit one part
  of the texture.

  After we created the texture, we just need to attach it to the framebuffer:

  #+NAME: Attaching created texture to a framebuffer
  #+BEGIN_SRC c
    glFramebufferTexture2D(GL_FRAMEBUFFER, GL_COLOR_ATTACHMENT0, GL_TEXTURE_2D, texture, 0);
  #+END_SRC

  _NOTE_: we can also attach a depth and/or stencil buffer. We can combine both using GL_DEPTH_STENCIL_ATTACHMENT.

- *Renderbuffer objects attachments*: it's also a regular buffer, just like texture attachments, but way faster for write
  operations, although very slow for reading. This is because this type of attachment doesn't work with texture formats
  or anything, it's manipulated at memory level (bytes), which allows OpenGL to perform optimizations. We can read from
  it if we want using the glReadPixels function, but it's a _slow_ operation.

  Renderbuffer objects can be moved around very fast. Swapping and copying buffers of this kind is cheap.

  #+NAME: Binding to a renderbuffer object and misc
  #+BEGIN_SRC c
    unsigned int rbo;
    glGenRenderbuffers(1, &rbo);
    glBindRenderbuffer(GL_RENDERBUFFER, rbo);
    glRenderbufferStorage(GL_RENDERBUFFER, GL_DEPTH24_STENCIL8, 800, 600);
    glFramebufferRenderbuffer(GL_FRAMEBUFFER, GL_DEPTH_STENCIL_ATTACHMENT, GL_RENDERBUFFER, rbo);
  #+END_SRC

Renderbuffer objects are generally used with depth and stencil buffers when we don't need to sample them. There are
situations where these two buffers need to get sampled, e.g for [[Shadow Mapping][shadow mapping]] or depth-based post-processing effects.

_NOTE_: Renderbuffer objects can be more efficient for use in your off-screen render projects, but it is important to
realize when to use renderbuffer objects and when to use textures. The general rule is that if you never need to sample
data from a specific buffer, it is wise to use a renderbuffer object for that specific buffer. If you need to sample
data from a specific buffer like colors or depth values, you should use a texture attachment instead.

**** Post-processing
If we use [[Attachment][texture attachments]], we can do some really cool effects while using the texture attachment in our fragment
shader.

Some of these effects are:

- Inversion: inverts colours.

- Gray-scale: all colours are black, white and gray.

- Kernel effects: these ones are the most interesting ones. _Another great point of using a texture as an attachment is
  that we can query other parts of the scene that are not the fragment itself_, allowing us to grab, for example, a small
  area around the current fragment to tweak its colours a little bit, creating fuzzy visual effects. Popular kernel
  effects are: sharpen, blur and edge detection.

** Cubemaps
It's a special kind of texture that is formed of six other textures.

Each texture represents a side of a cube, and all of them together form a single cube.

_These special textures have the property that can be indexed/sampled using a direction vector_. The magnitude of such
direction vector doesn't matter, OpenGL only cares about its direction. As long as we supply the direction, OpenGL is
able to retrieve the corresponding texel value.

For cubes and as long as they're centered on the origin, we can use its coordinates to access the right face of the
cubemap.

The process of creating a cubemap is similar to any other texture:

#+NAME Creating a cubemap
#+BEGIN_SRC c
  unsigned int textureID;
  glGenTextures(1, &textureID);
  glBindTexture(GL_TEXTURE_CUBE_MAP, textureID);
#+END_SRC

Then, we need to specify the right image for each side (6) of the cubemap:

#+NAME Creating a cubemap
#+BEGIN_SRC c
  int width, height, nrChannels;
  unsigned char *data;
  for(unsigned int i = 0; i < textures_faces.size(); i++) {
      data = stbi_load(textures_faces[i].c_str(), &width, &height, &nrChannels, 0);
      glTexImage2D(GL_TEXTURE_CUBE_MAP_POSITIVE_X + i,
                   0, GL_RGB, width, height, 0, GL_RGB, GL_UNSIGNED_BYTE, data);
   }
#+END_SRC

Don't forget to specify its wrapping and filtering methods:

#+NAME Creating a cubemap
#+BEGIN_SRC c
  glTexParameteri(GL_TEXTURE_CUBE_MAP, GL_TEXTURE_MAG_FILTER, GL_LINEAR);
  glTexParameteri(GL_TEXTURE_CUBE_MAP, GL_TEXTURE_MIN_FILTER, GL_LINEAR);
  glTexParameteri(GL_TEXTURE_CUBE_MAP, GL_TEXTURE_WRAP_S, GL_CLAMP_TO_EDGE);
  glTexParameteri(GL_TEXTURE_CUBE_MAP, GL_TEXTURE_WRAP_T, GL_CLAMP_TO_EDGE);
  glTexParameteri(GL_TEXTURE_CUBE_MAP, GL_TEXTURE_WRAP_R, GL_CLAMP_TO_EDGE);
#+END_SRC

The rest, that is, activating and binding the cubemap before rendering it, is the same as with any other texture.

The only thing that changes is that in our shaders the texture is a samplerCube, not a sampler2D.

*** Skybox
This is a perfect use-case for cubemaps. It's a large cubemap that encompasses the whole screen, giving the illusion of
an endless world.

It's a bit tricky to get it to work correctly, though.

What we want to do is to center the skybox around the player's position every frame, giving them the sensation of this
very large world. If we were to just render the skybox, it would get closer to the player when the player moves because
of how the view matrix transforms all of the skybox's positions by rotating, scaling and translating them.

_So, to accomplish this, what we need to do is to remove the translation section of the transformation matrices by
taking the upper-left 3x3 matrix of the 4x4 matrix_. This is done like this:

#+NAME Getting the upper-left 3x3 matrix of the 4x4 view matrix.
#+BEGIN_SRC cpp
  glm::mat4 view = glm::mat4(glm::mat3(camera.GetViewMatrix()));
#+END_SRC

_This removes any translation and keeps the rotation factor within the matrix, so the user can look around_.

Initially, the skybox was rendered first, which meant the computer had to calculate and draw every pixel on the screen,
even though only a small part of the skybox would be visible. This was not very efficient because it used up a lot of
processing power.

To make it more efficient, the skybox is now rendered last. This means that by the time the skybox is rendered, the
computer already knows where all the other objects in the scene are, so it can skip drawing parts of the skybox that are
behind these objects. This is done by using a technique that makes the skybox appear to be very far away, so it only
gets drawn in areas where there are no other objects.

However, there's a problem: the skybox is a small cube, so it might still appear in front of other objects because it's
very close to the camera. To solve this, the depth buffer (which keeps track of how far away objects are) is tricked
into thinking the skybox is very far away. This is done by setting the depth value of the skybox to the maximum possible
value, which is 1.0. This way, the skybox will only be drawn in areas where there are no other objects in front of it,
making the rendering process more efficient

This is done in the vertex shader:

#+NAME Changing skybox's cube Z component to be its W.
#+BEGIN_SRC glsl
void main() {
    TexCoords = aPos;
    vec4 pos = projection * view * vec4(aPos, 1.0);
    gl_Position = pos.xyww;
}
#+END_SRC

We'd also have to change the depth function for it to be: GL_LEQUAL instead of the default GL_LESS.

*** Environment Mapping
It's the technique of simulating reflective or refractive properties of an objects' surroundings on its surface using [[Cubemaps][cubemaps]].

**** Reflection
It's a phenomenom that happens where light bounces off a surface of an object. To simulate reflection in OpenGL, we need
four things:

1. A cubemap, which is the object that represents the environment of that object. It has to have a texture attached to
   it.
2. A vector that represents the viewer's direction towards the object. This is computed as the difference of fragment's
   position minus the camera's position. (I)
3. A normal vector that is perpendicular to the object's surface. (N)
4. The reflection vector, which is computed as the difference of the viewer's direction vector minus the normal
   vector. This vector points in the direction that light bounces off the surface and it will point to the right
   cubemap's texel. (R)

After computing the reflection vector, the only thing we need to do in our fragment shader is to sample the
corresponding texel of the cubemap, which will give us the right colour to show in the object's surface.

Here's the code that calculates reflection:

#+NAME Computing reflection, fragment shader
#+BEGIN_SRC glsl
  #version 330 core
  out vec4 FragColor;

  in vec3 Normal;
  in vec3 Position;

  uniform vec3 cameraPos;
  uniform samplerCube skybox;

  void main() {
      vec3 I = normalize(Position - cameraPos);
      vec3 R = reflect(I, normalize(Normal));
      FragColor = vec4(texture(skybox, R).rgb, 1.0);
  }
#+END_SRC

Because we're dealing with world-space coordinates here, we have to keep that in mind in our fragment vertex shader:

#+NAME Computing reflection, vertex shader
#+BEGIN_SRC glsl
  #version 330 core
  layout (location = 0) in vec3 aPos;
  layout (location = 1) in vec3 aNormal;

  out vec3 Normal;
  out vec3 Position;

  uniform mat4 model;
  uniform mat4 view;
  uniform mat4 projection;

  void main() {
      Normal = mat3(transpose(inverse(model))) * aNormal;
      Position = vec3(model * vec4(aPos, 1.0));
      gl_Position = projection * view * vec4(Position, 1.0);
  }
#+END_SRC

#+CAPTION: Reflection.png
#+NAME: Reflection.png
[[./Reflection.png]]

**** Refraction
It's a phenomenom that happens where light rays changes it's direction when they pass through an object's
material. Depending on the material, the direction could vary significantly or not.

The lower the _refractive index_, the less the light rays will disperse after going through the material.

Refraction is explained by _Snell's Law_.

The information that we need to simulate refraction is the same as [[Reflection][reflection]]. The only thing that changes is that we
use the _refract_ function instead of _reflect_.

#+NAME Computing refraction, fragment shader
#+BEGIN_SRC glsl
  void main() {
      float ratio = 1.00 / 1.52; // 1.00 -> air, 1.52 -> glass
      vec3 I = normalize(Position - cameraPos);
      vec3 R = refract(I, normalize(Normal), ratio);
      FragColor = vec4(texture(skybox, R).rgb, 1.0);
  }
#+END_SRC

Materials ratios are given in this table:

#+CAPTION: RefractiveIndices.png
#+NAME: RefractiveIndices.png
[[./RefractiveIndices.png]]

#+CAPTION: Refraction.png
#+NAME: Refraction.png
[[./Refraction.png]]

*** Geometry Shader
It's a stage within the graphics processing pipeline that comes after the _vertex shader_ and before the _fragment
shader_. Like in those two, we can use our own shaders in here.

This stage receives as input a collection of vertices -that form a primitive- and its goal is to generate shapes based
on those vertices.

Sometimes, this stage also generates or edits vertices and shapes to improve the overall performance of the pipeline.

In some cases, we can also use this stage to create cool effects, for example:

- Making objects explode/implode.
- Drawing normals for debugging.

#+NAME Example of basic geometry shader
#+BEGIN_SRC glsl
#version 330 core
layout (points) in;
layout (line_strip, max_vertices = 2) out;

void main() {
    gl_Position = gl_in[0].gl_Position + vec4(-0.1, 0.0, 0.0, 0.0);
    EmitVertex();

    gl_Position = gl_in[0].gl_Position + vec4( 0.1, 0.0, 0.0, 0.0);
    EmitVertex();

    EndPrimitive();
}
#+END_SRC

Let's explain this code snippet:

- *layout(points) in*: it specifies the type of primitive input for our geometry shader. In this case, by specifying
  _points_, we're saying that the callee is using GL_POINTS when drawing primitives. In other words, input vertices form
  points in the scene.

  There are different options:

  - _points_: when drawing GL_POINTS.
  - _lines_: when drawing GL_LINES or GL_LINE_STRIP.
  - _lines_adjacency_: when drawing GL_LINES_ADJACENCY or GL_LINE_STRIP_ADJACENCY.
  - _triangles_: when drawing GL_TRIANGLES, GL_TRIANGLE_STRIP or GL_TRIANGLE_FAN.
  - _triangles_adjacency_: when drawing GL_TRIANGLES_ADJACENCY or GL_TRIANGLE_STRIP_ADJACENCY.

- *layout(line_strip, max_vertices = 2) out*: it specifies the type of output we're going to produce and also the maximum
  amount of vertices. It can have these values:

  - points
  - line_strip
  - triangle_strip

- *gl_in[]*: it contains the output of the previous shader stage, namely, the vertex shader. It's an array that contains
  the vertices that form a single primitive. Its components are:

  - *gl_Position*: it contains the vector position of the vertex that was produced in the vertex shader.
  - *gl_PointSize*: it contains the size of the vertex.
  - *gl_ClipDistance[]*: to perform user-defined clipping.

- *EmitVertex()*: it's a function that allows us to create new vertices. The new vertex is created with the current values
  of gl_Position.

- *EndPrimitive()*: all emitted vertices with EmitVertex() and now combined to form the primitive.

_A common use of the geometry shader is to output object's normals. This is a good debugging technique because most of
the times visual bugs are related with incorrect normals._

How do we do that?

- Draw the scene as normal, without the geometry shader.
- Draw the scene again, but only displaying the normal vectors that we'll generate in the geometry shader. This geometry
  shader takes as input a triangle and generates 3 lines from them in the direction of their normals.

_The data that the geometry shader receives is in view-space coordinates!_

This would be the corresponding vertex shader, which outputs the normal vector for the geometry shader to use:

#+NAME Vertex shader for drawing normals for debugging purposes
#+BEGIN_SRC glsl
  #version 330 core
  layout (location = 0) in vec3 aPos;
  layout (location = 1) in vec3 aNormal;

  out VS_OUT {
      vec3 normal;
  } vs_out;

  uniform mat4 view;
  uniform mat4 model;

  void main() {
      gl_Position = view * model * vec4(aPos, 1.0); 
      mat3 normalMatrix = mat3(transpose(inverse(view * model)));
      vs_out.normal = normalize(vec3(vec4(normalMatrix * aNormal, 0.0)));
  }
#+END_SRC

#+NAME Geometry shader for drawing normals for debugging purposes
#+BEGIN_SRC glsl
#version 330 core
layout (triangles) in;
layout (line_strip, max_vertices = 6) out;

in VS_OUT {
    vec3 normal;
} gs_in[];

const float MAGNITUDE = 0.4;

uniform mat4 projection;

void GenerateLine(int index) {
    gl_Position = projection * gl_in[index].gl_Position;
    EmitVertex();
    gl_Position = projection * (gl_in[index].gl_Position + 
                                vec4(gs_in[index].normal, 0.0) * MAGNITUDE);
    EmitVertex();
    EndPrimitive();
}

void main() {
    GenerateLine(0); // first vertex normal
    GenerateLine(1); // second vertex normal
    GenerateLine(2); // third vertex normal
}
#+END_SRC

#+NAME Fragment shader for drawing normals for debugging purposes
#+BEGIN_SRC glsl
#version 330 core
out vec4 FragColor;

void main() {
    FragColor = vec4(1.0, 1.0, 0.0, 1.0);
}
#+END_SRC

*** Instancing
It's an optimization technique that's used when drawing a collection of objects where the mesh data is the same but
their location, colours, scalation, rotation or something else is different.

For example, if we were draw a forest where all trees share the same meshes, the only thing that really changes is the
location where we have to put each tree. If we don't use instancing and have, say, 1000 trees, then we'd make 1000 calls
to the shader, which has a performance penalty.

With instancing, however, we'd only make a single call, reducing CPU-GPU communications.

_How can we do instancing?_

The first thing to do is change these function calls:

- glDrawArrays -> glDrawArraysInstanced
- glDrawElements -> glDrawElementsInstanced

The second thing we need to do is using the global variable _gl_InstanceID_. This stores the current id of the instance
being drawn. This variable is accessible in the _vertex shader_.

The third thing we should keep in mind is using _instanced arrays_.

Instanced arrays come in handy when we need to pass different transformation data for each vertex. If we were to use a
uniform variable to store the data there, we'd reach a limit of the amount of data we can store in the shader pretty
soon. However, instanced arrays allows us to pass way more data than using uniforms.

They're also very good for performance because the vertex shader won't update the data per each vertex, but rather per
instance. That means that the GPU will only retrieve new transformation data (that vertex attribute) when the instance
changes, but not when the vertex changes.

#+NAME Example of setting up instanced arrays and instacing
#+BEGIN_SRC c
  // Create a VBO for the array instance.
  unsigned int instanceVBO;
  glGenBuffers(1, &instanceVBO);
  glBindBuffer(GL_ARRAY_BUFFER, instanceVBO);
  glBufferData(GL_ARRAY_BUFFER, sizeof(glm::vec2) * instancesNumber, &translations[0], GL_STATIC_DRAW);
  glBindBuffer(GL_ARRAY_BUFFER, 0);

  // Now we set the vertex attribute pointer.
  glEnableVertexAttribArray(2);
  glBindBuffer(GL_ARRAY_BUFFER, instanceVBO);
  glVertexAttribPointer(2, 2, GL_FLOAT, GL_FALSE, 2 * sizeof(float), (void*)0);
  glBindBuffer(GL_ARRAY_BUFFER, 0);
  glVertexAttribDivisor(2, 1); // This tells OpenGL when to update the contents of the vertex attribute to the next element.
#+END_SRC

The key here is the function _glVertexAttribDivisor_: it tells OpenGL when to update the contents of the vertex attribute
to the next element. It has two arguments:

- First argument: the vertex attribute in question.
- Second argument: it's the _attribute divisor_. By default, the attribute divisor is 0 which tells OpenGL to update the
  content of the vertex attribute each iteration of the vertex shader. By setting this attribute to 1 we're telling
  OpenGL that we want to update the content of the vertex attribute when we start to render a new instance. By setting
  it to 2 we'd update the content every 2 instances and so on.

_NOTE_: when we declare a vertex attribute whose size is greater than glm::vec4, then we have to split it up in different
vertex attributes. For example, if we have to pass a glm::mat4, we'd have to create 4 different vertex attributes.

*** Anti Aliasing
Aliasing is the effect that we can see in some games where object's borders sometimes look with a jagged saw-like
pattern.

These jagged edges appear because of how the rasterizer transforms the vertex data into fragments.

The techniques that try to solve this problem are called anti-aliasing techniques. Nowadays, we have two:

- Super sample anti-aliasing (SSAA): deprecated. Not used anymore because of performance reasons.
- Multisample anti-aliasing (MSAA): builds on top of SSAA.

**** MSAA
MSAA (Multisample Anti-Aliasing) is an anti-aliasing technique that enhances the visual quality of rendered images by
smoothing out jagged edges. It operates by rendering a scene at a higher internal resolution and then downscaling the
image to match the display resolution. This method is less computationally intensive than SSAA (Supersample
Anti-Aliasing), making it a more performance-friendly option.

In MSAA, multiple samples are taken per pixel to determine the final color of each pixel. Typically, this involves 2, 4,
or 8 samples per pixel. The process involves analyzing the pixel geometry edges and computing anti-aliasing values per
pixel before downscaling the image to the display resolution. The number of samples per pixel directly influences the
quality of the anti-aliasing effect, with higher sample counts generally resulting in smoother edges but at the cost of
performance.

The technique works by using the coverage information (how many samples are covered by a pixel) to determine the final
colour. This process is known as resolving the render target. The resolve process typically involves averaging the
colours of the covered samples to produce the final colour for each pixel. It's important to note that MSAA doesn't
directly determine the colour of a pixel based on the number of covered sample points. Instead, it uses the coverage
information to determine the final colour.

The fragment shader, which is responsible for determining the colour of each pixel, operates on the resolved samples
rather than the original geometry. The fragment shader does run, but its input is the resolved colour information, not
the raw geometry data.

MSAA effectively creates a smooth edge effect by averaging the colours of the covered samples, which can reduce the
appearance of jagged edges in rendered images. However, it's worth noting that MSAA can introduce some blurring effects,
especially in scenes with high-frequency details or transparency.

In OpenGL we can use MSAA like this:

#+NAME Enabling MSAA for the default framebuffer.
#+BEGIN_SRC c
  glfwWindowHint(GLFW_SAMPLES, 4);
  glEnable(GL_MULTISAMPLE);
#+END_SRC

If we want to apply MSAA off-screen, we'd have to create our own framebuffer and generate our multisampled buffers.

We have two ways of doing this:

- Using multisampled texture attachments: we need to create our texture as usual first, and then bind it using
  GL_TEXTURE_2D_MULTISAMPLE:

#+NAME Binding a 2D texture with multisample.
#+BEGIN_SRC c
  glBindTexture(GL_TEXTURE_2D_MULTISAMPLE, tex);
  glTexImage2DMultisample(GL_TEXTURE_2D_MULTISAMPLE, samples /* 2, 4, 6, 8 */, GL_RGB, width, height, GL_TRUE);
  glBindTexture(GL_TEXTURE_2D_MULTISAMPLE, 0);
#+END_SRC

Then, to attach this texture to a framebuffer, we'd do:

#+NAME Attaching 2D texture with multisample to a framebuffer
#+BEGIN_SRC c
  glFramebufferTexture2D(GL_FRAMEBUFFER, GL_COLOR_ATTACHMENT0, GL_TEXTURE_2D_MULTISAMPLE, tex, 0);
#+END_SRC

- Using multisampled renderbuffer objects: it's the same as creating a regular renderbuffer object but we use
  glRenderbufferStorageMultisample instead of glRenderbufferStorage:

#+NAME Multisampled renderbuffer object
#+BEGIN_SRC c
  glRenderbufferStorageMultisample(GL_RENDERBUFFER, samples, GL_DEPTH24_STENCIL8, width, height);
#+END_SRC

_To render to our framebuffer, we need to:_

- Resolve the image: calling glBlitFramebuffer, which copies a region from one framebuffer to the other while also
  resolving any multisampled buffers.

#+NAME: Resolving the image
#+BEGIN_SRC c
  glBindFramebuffer(GL_READ_FRAMEBUFFER, multisampledFBO);
  glBindFramebuffer(GL_DRAW_FRAMEBUFFER, 0);
  glBlitFramebuffer(0, 0, width, height, 0, 0, width, height, GL_COLOR_BUFFER_BIT, GL_NEAREST); 
#+END_SRC

_NOTE_: if we want to use the texture result of a multisampled framebuffer to do post-processing, for example, then we'd
have to create an intermediate framebuffer object because we _can't use multisampled textures directly in the fragment
shader_.

This would be an example:

#+NAME: Using an intermediate framebuffer to store the multisampled texture to then use it on our fragment shader.
#+BEGIN_SRC c
unsigned int msFBO = CreateFBOWithMultiSampledAttachments();
// then create another FBO with a normal texture color attachment
[...]
glFramebufferTexture2D(GL_FRAMEBUFFER, GL_COLOR_ATTACHMENT0, GL_TEXTURE_2D, screenTexture, 0);
[...]
while(!glfwWindowShouldClose(window)) {
    [...]
    glBindFramebuffer(msFBO);
    ClearFrameBuffer();
    DrawScene();
    // now resolve multisampled buffer(s) into intermediate FBO
    glBindFramebuffer(GL_READ_FRAMEBUFFER, msFBO);
    glBindFramebuffer(GL_DRAW_FRAMEBUFFER, intermediateFBO);
    glBlitFramebuffer(0, 0, width, height, 0, 0, width, height, GL_COLOR_BUFFER_BIT, GL_NEAREST);
    // now scene is stored as 2D texture image, so use that image for post-processing
    glBindFramebuffer(GL_FRAMEBUFFER, 0);
    ClearFramebuffer();
    glBindTexture(GL_TEXTURE_2D, screenTexture);
    DrawPostProcessingQuad();
    [...]
}
#+END_SRC

** Mathematics
*** Basics of Trigonometry
Trigonometry means the study of triangles.

**** Pythagoras Theorem

\(\vec{c} = \sqrt{\vec{a}^2 + \vec{b}^2}\)

**** Acute Angle
Angles under 90º.

**** Angle of Elevation
When we see an object that's above us, there's an angle between our line of sight to that object and the horizontal line.

**** Angle of Depression
When we see an object that's below us, there's an angle between our line of sight to that object and the horizontal line.

**** Right Triangle
It's a triangle where at least one of its angles is 90º.

Example:

#+CAPTION: Right Triangle
#+NAME: RightTriangle.png
[[./RightTriangle.png]]

**** Very Important Trigonometry Functions
They're sine (sin), cosine (cos) and tangent (tan). They're known as trigonometric ratios.

Mnemonic: soh, cah, toa.

sine = opposite / hypotenuse
cosine = adjacent / hypotenuse
tangent = opposite / adjacent

They're all defined for [[Acute Angle][acute angles]].

**** Inverse Trigonometric Functions
If we need to find angles of triangles we need to make use of inverse trigonometric functions.

They do the opposite of trigonometric functions.

- Inverse Sine: \(\sin^{-1}(x)\). Also called arcsin.
- Inverse Cosine: \(\cos^{-1}(x)\). Also called arccos.
- Inverse Tangent: \tan^{-1}(x)\). Also called arctan.

_NOTE_: That -1 does not represent a power (that is called multiplicative inverse or reciprocal)!

**** Sine and Cosine of Complementary Angles

#+CAPTION: Sine and Cosine Complementary Angles
#+NAME: SineCosineComplementary.png
[[./SineCosineComplementary.png]]

**** Reciprocal Trigonometric Functions
They are cosecant, secant and cotagent. They're also ratios.

Cosecant: it's the reciprocal of the sine.

#+CAPTION: Cosecant
#+NAME: Cosecant.png
[[./Cosecant.png]]

Secant: it's the reciprocal of the cosine.

#+CAPTION: Secant
#+NAME: Secant.png
[[./Secant.png]]

Cotangent: it's the reciprocal of the tangent.

#+CAPTION: Cotangent
#+NAME: Cotangent.png
[[./Cotangent.png]]

**** Law of Sines and Cosines
Given a triangle where we know two angles and one of its sides, then we can use the law of sines and
cosines to "solve" the rest of the triangle.

This shit is incredible useful for any kind of triangles.

We just use this motherfucking formula to get what we want (remember to always think relative to an angle):

#+CAPTION: Law of Sines
#+NAME: LawOfSines.png
[[./LawOfSines.png]]

#+CAPTION: Law of Cosines
#+NAME: LawOfCosines.png
[[./LawOfCosines.png]]

I'm not sure about this but I think the law of sines is best used when we know at least one side of the triangle and two
angles. For the law of cosines, we need to know two sides of the triangle and at least one angle or the three sides, to
then get a desired angle.
With that information we can solve the triangle completely.

The law of cosines is a bloody generalization of the [[Pythagoras Theorem][Pythagoras Theorem]].

*** Vectors
Are just directions. They have a direction and a magnitude.

We generally work with vectors from 2D to 4D.

The direction tells us where the vector is heading to: left, right, up, down, etc.

The magnitude (also known as strength or length) tells us how many "steps" or "units" we have to walk until we reach our
destination.

It's like a treasure map: (1, 3) means walk 1 step rightwards, then 3 steps upwards.

Because they're directions, we don't care about the origin of them to define equality. For example, these two vectors w
and v are equal:

#+CAPTION: Equal vectors
#+NAME: EqualVectors.png
[[./EqualVectors.png]]

Vectors in formulas are represented like this:

#+CAPTION: Vector Representation
#+NAME: VectorDefinition.png
[[./VectorDefinition.png]]

When we want to represent a vector as a coordinate or position, we pick an origin, generally (0, 0, 0), and from there
we draw a line pointing to the position or point that we want. This is called a _position vector_.
We could also pick some other origin and then say: "this vector points to that point in space from this origin".

Using vectors we can represent _positions_ and _directions_ in 2D and 3D space.

**** Scalar Vector Operations
When we say scalar we just mean a constant. It's just about multiplying a vector by a constant.

Addition, subtraction, multiplication and division by a scalar consists on adding/subtract/multiply and divide each
vector component by that scalar, and that's it.

For example:

#+CAPTION: Vector Scalar Operations
#+NAME: VectorScalarOperations.png
[[./VectorScalarOperations.png]]

This has the effect of [[Scaling][scaling]] the vector by some amount. For example: zooming in and out.

**** Vector Negation
When we negate a vector we get the same vector but in the reversed direction.

If we have a vector that points to north-east and we negate it, we'd get a vector pointing to south-west.

It's just about flipping the sign of each vector component, which means multiplying it by -1!

#+CAPTION: Vector Negation
#+NAME: VectorNegation.png
[[./VectorNegation.png]]

**** Addition and Subtraction
Both operations are _component wise_:

#+CAPTION: Vector Addition and Subtraction
#+NAME: VectorAddSub.png
[[./VectorAddSub.png]]

If we had two vectors, v(4, 2) k(1, 2) and we'd add them together, we'd get a new vector that points directly to the end
point, something like this:

#+CAPTION: Adding Two Vectors Representation
#+NAME: AddingTwoVectors.png
[[./AddingTwoVectors.png]]

Related stuff: [[Parallelogram Law][parallelogram law]].

Subtraction yields another vector that's the difference between the two points the vectors are pointing at.
In other words, we get a vector that points from the end of the first vector to the end of the second vector.
Like this:

#+CAPTION: Subtracting Two Vectors Representation
#+NAME: SubtractingTwoVectors.png
[[./SubtractingTwoVectors.png]]

**** Length
It can be understood as the distance or units we have to travel from an origin to the end point of a vector. This
distance is in a straight line, meaning it's the shortest distance to get to that end point.

We obtain it using Pythagoras theorem, because we can represent the vector as a rectangle triangle:

#+CAPTION: Magnitude or Length of a Vector
#+NAME: MagnitudeLengthVector.png
[[./MagnitudeLengthVector.png]]

We can even extend this to 3D by adding the \(^{z^2}\) to the equation.

**** Unit Vector
It's just a vector but with an important property: it's length is 1.

This happens after [[Normalizing Vectors][normalizing]] a vector.

We can calculate a unit vector from any vector by dividing each component by its length, like this:

\(\hat{n} = \frac{\vec{v}}{\|\vec{v}\|}\)

**** Normalizing Vectors
Normalizing a vector means dividing each vector's component by the length of the vector.

By doing this, we'll get a new vector that points in the same direction but with length 1.

It's particularly useful when comparing different vectors.

But what does it mean to normalize a vector? Here's an example:

Imagine you have a bunch of arrows, each pointing in a different direction and with different lengths. Some arrows are
longer, and some are shorter. Now, let's say you want to compare how strong each arrow is in pushing something. But the
problem is, it's hard to tell how strong they are just by looking at them because some are longer than others.

So, to make it fair and easier to compare, we decide to make all the arrows the same length. We don't care about their
original lengths; we just want to see how strong they are compared to each other when they're all the same length.

In math, vectors are like those arrows, and their 'strength' is how long they are. When we normalize a vector, we're
making all the vectors the same length, like making all the arrows the same length. This makes it easier to compare and
work with vectors in math and computer graphics.

**** Vector-to-Vector Multiplication
There's two ways of multiplying matrices: dot and cross product.

***** Dot Product
The dot product of two vectors is the product of their lengths times the cosine of the angle between them.

\(\vec{v} \cdot \vec{k} = \|\vec{v}\| \cdot \|\vec{k}\| \cdot \cos \theta\)

If v and k were [[Unit Vector][unit vectors]] then we can say that _the dot product of two vectors defines the angle between them_.

The letter (θ) is theta.

Considering that the cosine of 90º is 0 and the cosine of 0º is 1, then we could _test, using the dot product, if two
vectors are parallel or orthogonal to each other_. Orthogonal means perpendicular, which means that the two vectors form
a right angle (90º).

It's just a _component-wise_ multiplication where we add the results together.

#+CAPTION: Dot Product
#+NAME: DotProduct.png
[[./DotProduct.png]]

Having the dot product and knowing that theta can be obtained with the inverse of the cosine, then we have:

\(\theta = \cos^{-1}(\vec{a} \cdot \vec{b})\)
\(\theta = \cos^{-1}(-0.8)\)

Which yields a degree of 143,1.

***** Cross Product
It's only defined in 3D space.

The output of this operation is a new vector that is orthogonal to both.

#+CAPTION: Cross Product Representation
#+NAME: CrossProduct.png
[[./CrossProduct.png]]

This is the formula. To really understand what's going on I should learn more linear algebra:

#+CAPTION: Cross Product Formula
#+NAME: CrossProductFormula.png
[[./CrossProductFormula.png]]

*** Matrices
They're a rectangular array of numbers. Classic NxM array. NxM are called the _dimensions_ of the matrix.

Each number is called an _element_ of the matrix.

**** Matrix Addition and Subtraction
Additions and subtractions are only defined for matrices with the same dimensions.

They're _element-wise_ operations.

#+CAPTION: Addition and Subtraction of Matrices
#+NAME: AddSubMatrices.png
[[./AddSubMatrices.png]]

**** Matrix-scalar products
It's similar to vectors. We multiply the scalar times each component:

#+CAPTION: Matrix-scalar product
#+NAME: MatrixScalarMultiplication.png
[[./MatrixScalarMultiplication.png]]

**** Matrix-Matrix Multiplication
There are restrictions to multiply matrices:

1. _We can only multiply matrices if the number of columns of the left-hand side matrix is equal to the number of rows of
   the right-hand side matrix_.
2. _It's not commutative, meaning A · B != B · A_.

Example:

#+CAPTION: Matrix-Matrix Multiplication
#+NAME: MatrixMatrixMultiplication.png
[[./MatrixMatrixMultiplication.png]]

The output matrix is one with (N, M) dimensions, where N is the number of rows of the left-hand side matrix and M is the
number of columns of the right-hand side matrix.
-Vector Multiplication
If we have a MxN matrix and we have a Nx1 vector, multiplication is defined.

There are interesting properties when we perform these kind of multiplications.

_Multiplying that matrix with the vector transforms the vector!_

**** Identity Matrix
One _transformation_ matrix that we can think of is the _identity matrix_.

This matrix is just a NxN matrix with all 0's except on its diagonal, which is filled with 1's.

When we multiply a vector by this matrix, it leaves the vector unchanged:

#+CAPTION: Identity Matrix
#+NAME: IdentityMatrix.png
[[./IdentityMatrix.png]]

**** Scaling
There are two types of scaling: _non-uniform_ and _uniform_.

- Non-uniform: when we scale the matrix with different factors per axis.
- Uniform: when we scale the matrix with the same factor per axis.

Now, considering the identity matrix, we can scale a vector by changing the 1's in the diagonal with the factors we
want:

#+CAPTION: Scaling Vector With Matrix
#+NAME: ScalingVectorWithMatrix.png
[[./ScalingVectorWithMatrix.png]]

We let the 4th component to 1 for now. This corresponds to the W component in OpenGL.

**** Translation
It's adding another vector on top of the original vector. This yields a new vector with a different position.

This means we're "moving" the original vector based on a translation vector.

Again, we use the identity matrix to perform this operation. In this case and because we have 4 components (it wouldn't
work with 3) we can do this trick:

#+CAPTION: Translation Matrix
#+NAME: TranslationMatrix.png
[[./TranslationMatrix.png]]

Where T's are the components of the translation vector and x,y,z is the original vector.

With a translation matrix we can move any object in the 3 axis directions.

**** Homogeneous Coordinates
The W component of a vector is known as a homogeneous coordinate.

If we want to get the 3D vector of a homogeneous vector (4D), we need to divide each component by the W coordinate.

Usually this component has the value 1.0.

It helps us translate 3D vectors and to create 3D perspective.

If the W coordinate is 0.0, then the vector is known to be a _direction vector_ because it cannot be translated.

**** Rotation
Rotation means rotating an object over an axis with a certain angle.

To rotate 2D objects in OpenGL, we only need to provide the angle and the rotation axis as the Z-axis.

To rotate 3D objects, we need to provide the angle and the rotation axis (the one that will remain fixed over the
duration of the rotation).

To transform vectors to rotate them (rotated vectors), depending on the axis we want to rotate on, we can use these
matrices:

Rotation around the X-axis:

#+CAPTION: Rotation Matrix X Axis
#+NAME: RotationMatrixXAxis.png
[[./RotationMatrixXAxis.png]]

Rotation around the Y-axis:

#+CAPTION: Rotation Matrix Y Axis
#+NAME: RotationMatrixYAxis.png
[[./RotationMatrixYAxis.png]]

Rotation around the Z-axis:

#+CAPTION: Rotation Matrix Z Axis
#+NAME: RotationMatrixZAxis.png
[[./RotationMatrixZAxis.png]]

NOTE: To rotate around an arbitrary 3D axis we'd need to use a more complex matrix or quaternions.

NOTE: Keep in mind that the axis that we rotate around should be a [[Unit Vector][unit vector]], so be sure to normalize the vector first
if you're not rotating around the X, Y, or Z axis.

*** Combining Matrices
Matrices are awesome because we can mix multiple transformation matrices in a single matrix to then modify a vector.

For example, say we have our vector (x, y, z) and we wanted to scale it by 2 and then translate it by (1, 2, 3).

Our combination of scale and translate matrix would look like this:

#+CAPTION: Transformation Matrix Combination
#+NAME: TransformationMatrixCombination.png
[[./TransformationMatrixCombination.png]]

It's important _to remember that matrix multiplication is not commutative_, so we first translate and then scale.

It's important to read the multiplication from right to left (math. properties).

NOTE: _It is advised to first do scaling operations, then rotations and lastly translations when combining matrices
otherwise they may (negatively) affect each other_.

You can see in this image that the vector is firstly scaled by 2 and then translated by 1, 2 and 3!

#+CAPTION: Transformation Matrix Combination End
#+NAME: TransformationMatrixCombinationEnd.png
[[./TransformationMatrixCombinationEnd.png]]

*** Parallelogram Law
Imagine two vectors starting from the same point. This law says that if we place these two vectors tail to tail, they
will form two sides of a parallelogram. The sum of these two vectors is represented by a diagonal of the parallelogram
that starts at the same point as the two vectors.

#+CAPTION: Parallelogram Law
#+NAME: Parallelogram Law
[[./ParallelogramLaw.png]]

*** Read, TODO
In order:
- https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab
- http://www.songho.ca/opengl/gl_projectionmatrix.html
- https://en.wikipedia.org/wiki/Gram%E2%80%93Schmidt_process
- https://www.youtube.com/watch?v=4DQquG_o-Ac

We need to understand the explanation and REALLY how the two exercises of the "Transformations" chapter work. If we
don't, we'll never get good at this.

- http://www.lighthouse3d.com/tutorials/glsl-12-tutorial/the-normal-matrix/

* Normal Vector
It's a unit vector that is perpendicular to the surface of a vertex. Because a vertex doesn't have a surface (it's just
a point), we compute the normal vector by using its surrounding vertices.

TODO: process. not explained yet.

* Wavefront Format
It's a format that tools like Blender, 3DMax or Maya use to export models. The extension is .obj.

Files stored in this format contain:

- Vertices.
- Normals.
- Texture coordinates.

* Collada
It's also a file format that tools like Blender, 3DMax or Maya use to export models. This is way more flexible than the
[[Wavefront
Format][wavefront format]], but this one is harder to parse.

Files stored in this format can contain:

- Vertices.
- Normals.
- Texture coordinates.
- Models.
- Lights.
- Materials.
- Animation data.
- Cameras.
- Scene information.

* Samples
TODO

* Multisampling
TODO
